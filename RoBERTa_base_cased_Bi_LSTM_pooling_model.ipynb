{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"RoBERTa_base_cased_Bi_LSTM_pooling_model.ipynb","provenance":[{"file_id":"1-fJsrXsuLr9fRzV-k7Dk2-9pD0KPD9L1","timestamp":1607198858784},{"file_id":"10TWl8ZEOK4aFpXNxKGQ8sNTDe8TbnYee","timestamp":1606471759418},{"file_id":"1XAThkmGIHNtA3Zh2swU8WUNe1CRiwCwO","timestamp":1606250659791}],"collapsed_sections":[],"machine_shape":"hm"},"coursera":{"schema_names":["NLPC3-4A"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"93826d389f8b45b08b7810d0356bee61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fe35fa65907b489da363daab757acb54","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_456e5bd59f5f459fa7eb04c8a5063cf6","IPY_MODEL_84a4d8f6898440348c135db2d33499ec"]}},"fe35fa65907b489da363daab757acb54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"456e5bd59f5f459fa7eb04c8a5063cf6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cb98d28e8a5b42c4ae04e3ccd23a619d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":481,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":481,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_90d4a2b30b424fce88dc635ad5de6bd2"}},"84a4d8f6898440348c135db2d33499ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f36cba909ea43a5a031dfec7ab9e83c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 481/481 [00:15&lt;00:00, 31.0B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e33be68bee0f4f999b0e824fe2e3d6ec"}},"cb98d28e8a5b42c4ae04e3ccd23a619d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"90d4a2b30b424fce88dc635ad5de6bd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f36cba909ea43a5a031dfec7ab9e83c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e33be68bee0f4f999b0e824fe2e3d6ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3884e00761804355b126c9e206bf0543":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f51592efdfec4e958bb89137bba6bdf1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b00a8de65871463085ba1751bfdfde6c","IPY_MODEL_d094c30d6e4042a78d47299553164b33"]}},"f51592efdfec4e958bb89137bba6bdf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b00a8de65871463085ba1751bfdfde6c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_615434062c784393a4e7fc3319871d0d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":657434796,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":657434796,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ec9b239d9ba14dd0bf3a5def5b52fa39"}},"d094c30d6e4042a78d47299553164b33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4b009cb06e174f51821c12792b1f237c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 657M/657M [00:14&lt;00:00, 44.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_824664bdc46942eaacb8407d470d02d2"}},"615434062c784393a4e7fc3319871d0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ec9b239d9ba14dd0bf3a5def5b52fa39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b009cb06e174f51821c12792b1f237c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"824664bdc46942eaacb8407d470d02d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab381db9e1874602a3bc993e682c3b2d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_68320a4e7a2d410ba31a19112001e7dc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b1623f51ee094b4f9fbd02dd5cc4f5a5","IPY_MODEL_2c4ee3d14a2a4b2499746af97fa689d2"]}},"68320a4e7a2d410ba31a19112001e7dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b1623f51ee094b4f9fbd02dd5cc4f5a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_99a084bc648b442db4f221266d628fdd","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_860972196ba14c86a3005dce545fee8c"}},"2c4ee3d14a2a4b2499746af97fa689d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a7024971eb1341fca37d562de216ed10","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 899k/899k [00:00&lt;00:00, 2.71MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df2db500f1684e2f993feedea32ef94e"}},"99a084bc648b442db4f221266d628fdd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"860972196ba14c86a3005dce545fee8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a7024971eb1341fca37d562de216ed10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"df2db500f1684e2f993feedea32ef94e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"47b49953aef744a2a05a3fa1ebd0768b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_63a219c4bbc44d00ab81619570879290","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_732dd4b9b6574b40bb7cce7f54a09a85","IPY_MODEL_65f49d8f1aea44d3951b90d5bb5f4e78"]}},"63a219c4bbc44d00ab81619570879290":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"732dd4b9b6574b40bb7cce7f54a09a85":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0104d11d9cc54b2da453fb416268bbc5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_05866dd8f1d841d9832dfe3cbd8c012a"}},"65f49d8f1aea44d3951b90d5bb5f4e78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4be30683d6394bff965b8ebca3bba32e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 3.81MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2c5c8ad1f3b94e6eb42fa3971b30b680"}},"0104d11d9cc54b2da453fb416268bbc5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"05866dd8f1d841d9832dfe3cbd8c012a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4be30683d6394bff965b8ebca3bba32e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2c5c8ad1f3b94e6eb42fa3971b30b680":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"PYBI0Mi-j07D"},"source":["#!cat /proc/cpuinfo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llTFOgQy9iYj","executionInfo":{"status":"ok","timestamp":1613284244945,"user_tz":-360,"elapsed":1785,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"c7c6968c-b5c1-4688-8f29-797534462416"},"source":["# connected GPU information\n","!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sun Feb 14 06:30:45 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jdG8TWva4fpa","executionInfo":{"status":"ok","timestamp":1613284276265,"user_tz":-360,"elapsed":26004,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"59b229cd-0c74-4705-cb11-e501b84314f2"},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XhY0C80ISa-F","executionInfo":{"status":"ok","timestamp":1613284314426,"user_tz":-360,"elapsed":8502,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"f7c9e654-5f7b-49de-a23b-3ffd3d1774c8"},"source":["# install huggingface transformers\n","!pip -q install transformers"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.8MB 5.6MB/s \n","\u001b[K     |████████████████████████████████| 890kB 36.7MB/s \n","\u001b[K     |████████████████████████████████| 3.2MB 47.4MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zdACgs491cs2","executionInfo":{"status":"ok","timestamp":1613284316315,"user_tz":-360,"elapsed":916,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}}},"source":["import os\n","\n","import tensorflow as tf\n","import transformers\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","\n","import random as rnd"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MB5Me0Ebrxad","executionInfo":{"status":"ok","timestamp":1613284321077,"user_tz":-360,"elapsed":3002,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"1bcd3721-9618-42ec-c1c2-170d22b733ca"},"source":["import re\r\n","import string\r\n","\r\n","import nltk\r\n","\r\n","from nltk.corpus import stopwords\r\n","from nltk.stem import PorterStemmer\r\n","\r\n","nltk.download('punkt')\r\n","nltk.download('stopwords')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Ui7sHLhTEq2D","executionInfo":{"status":"ok","timestamp":1613284321078,"user_tz":-360,"elapsed":2115,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}}},"source":["import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4sF9Hqzgwt0l"},"source":["<a name='1'></a>\n","#Importing the Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"sXWBVGWnpity","executionInfo":{"status":"ok","timestamp":1613284323624,"user_tz":-360,"elapsed":3465,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"53622f28-d614-463d-83fc-46caa81d175b"},"source":["data = pd.read_csv(\"/content/drive/My Drive/Duplicate Question Detection/quora_questions.csv\")\n","#data.fillna(\"none value\", inplace=True) # replace nan value to none\n","# drop the rows with null value\n","data.dropna(axis=0, inplace=True)\n","N=len(data)\n","print('Number of question pairs: ', N)\n","data.head()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Number of question pairs:  404348\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>qid1</th>\n","      <th>qid2</th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>is_duplicate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>What is the step by step guide to invest in sh...</td>\n","      <td>What is the step by step guide to invest in sh...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n","      <td>What would happen if the Indian government sto...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>How can I increase the speed of my internet co...</td>\n","      <td>How can Internet speed be increased by hacking...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>Why am I mentally very lonely? How can I solve...</td>\n","      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>Which one dissolve in water quikly sugar, salt...</td>\n","      <td>Which fish would survive in salt water?</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  qid1  ...                                          question2 is_duplicate\n","0   0     1  ...  What is the step by step guide to invest in sh...            0\n","1   1     3  ...  What would happen if the Indian government sto...            0\n","2   2     5  ...  How can Internet speed be increased by hacking...            0\n","3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n","4   4     9  ...            Which fish would survive in salt water?            0\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"yMLgpExkBY6g"},"source":["# def cut_to_max(text, max_len):\n","#     words = text.split()[:max_len]\n","#     return ' '.join(words)\n","\n","# # test the func\n","# # cut_to_max(' abc test wewe dfd ddd', 3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvFTS24At1y0","executionInfo":{"status":"ok","timestamp":1613284329860,"user_tz":-360,"elapsed":911,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}}},"source":["def cleaning_text(text):\r\n","    ''' Pre process and convert texts to a list of words '''\r\n","    text = str(text)\r\n","    #text = text.lower()\r\n","\r\n","    # Clean the text\r\n","    #text = re.sub(r\"[^A-Za-z0-9^,!.?\\/'+-=]\", \" \", text)\r\n","    text = re.sub(r\"what's\", \"what is \", text)\r\n","    text = re.sub(r\"What's\", \"What is \", text)\r\n","    text = re.sub(r\"\\'s\", \" \", text)\r\n","    text = re.sub(r\"\\'ve\", \" have \", text)\r\n","    text = re.sub(r\"can't\", \"cannot \", text)\r\n","    text = re.sub(r\"Can't\", \"Cannot \", text)\r\n","    text = re.sub(r\"n't\", \" not \", text)\r\n","    text = re.sub(r\"i'm\", \"i am \", text)\r\n","    text = re.sub(r\"I'm\", \"I am \", text)\r\n","    text = re.sub(r\"\\'re\", \" are \", text)\r\n","    text = re.sub(r\"\\'d\", \" would \", text)\r\n","    text = re.sub(r\"\\'ll\", \" will \", text)\r\n","    text = re.sub(r\",\", \" , \", text)\r\n","    text = re.sub(r\"\\.\", \" . \", text)\r\n","    text = re.sub(r\"\\?\", \" ? \", text)\r\n","    text = re.sub(r\"!\", \" ! \", text)\r\n","    text = re.sub(r\"\\/\", \" \", text)\r\n","    text = re.sub(r\"\\^\", \" ^ \", text)\r\n","    text = re.sub(r\"\\+\", \" + \", text)\r\n","    text = re.sub(r\"\\-\", \" - \", text)\r\n","    text = re.sub(r\"\\=\", \" = \", text)\r\n","    text = re.sub(r\"'\", \" \", text)\r\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\r\n","    text = re.sub(r\":\", \" : \", text)\r\n","    text = re.sub(r\" e g \", \" eg \", text)\r\n","    text = re.sub(r\" b g \", \" bg \", text)\r\n","    text = re.sub(r\" u s \", \" american \", text)\r\n","    text = re.sub(r\"\\0s\", \"0\", text)\r\n","    text = re.sub(r\" 9 11 \", \"911\", text)\r\n","    text = re.sub(r\"e - mail\", \"email\", text)\r\n","    text = re.sub(r\"j k\", \"jk\", text)\r\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\r\n","\r\n","    return text"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"deYmlI46IvjG","executionInfo":{"status":"ok","timestamp":1613284358538,"user_tz":-360,"elapsed":24922,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}}},"source":["data['question1'] = data['question1'].apply(lambda x: cleaning_text(x))\n","data['question2'] = data['question2'].apply(lambda x: cleaning_text(x))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"da9RE9hnMcej","executionInfo":{"status":"ok","timestamp":1613284358539,"user_tz":-360,"elapsed":23342,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}}},"source":["# for s in data['question1']:\n","#     if len(s.split()) > 63:\n","#         print(s)\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ktQColfXYdXS","executionInfo":{"status":"ok","timestamp":1613284358540,"user_tz":-360,"elapsed":22902,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"a2194694-c03d-44c5-92df-a1d409d6d078"},"source":["print(\"dataset labels Distribution\")\n","print(data.is_duplicate.value_counts())"],"execution_count":13,"outputs":[{"output_type":"stream","text":["dataset labels Distribution\n","0    255042\n","1    149306\n","Name: is_duplicate, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gkSQTu7Ypit0"},"source":["We first split the data into a train and test set. The test set will be used later to evaluate our model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkKiI0gip6Qs","executionInfo":{"status":"ok","timestamp":1613284358541,"user_tz":-360,"elapsed":17612,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"2ab2725d-ade8-46ca-a79d-74a372c972e0"},"source":["# model selection\n","from sklearn.model_selection import train_test_split\n","data_train , data_test = train_test_split(data, train_size=0.75, random_state=0)\n","# validation_data, _ = train_test_split(data_train, train_size=0.1, random_state=0)\n","len(data_train), len(data_test)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(303261, 101087)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"vJQespJVZJKC"},"source":["# print(\"Train dataset Distribution\")\n","# print(data_train.is_duplicate.value_counts())\n","\n","# print(\"\\n\\nTest dataset Distribution\")\n","# print(data_test.is_duplicate.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHpZO58Dss_v","executionInfo":{"status":"ok","timestamp":1613284358542,"user_tz":-360,"elapsed":10536,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}}},"source":["train_Q1 = np.array(data_train['question1'])\n","train_Q2 = np.array(data_train['question2'])\n","t_labels = np.array(data_train['is_duplicate'])\n","# make it one-hot encoding\n","train_labels = tf.keras.utils.to_categorical(t_labels, num_classes=2)\n","\n","test_Q1 = np.array(data_test['question1'])\n","test_Q2 = np.array(data_test['question2'])\n","te_labels  = np.array(data_test['is_duplicate'])\n","# make it one-hot encoding\n","test_labels = tf.keras.utils.to_categorical(te_labels, num_classes=2)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6t-fu-fL-qf4","executionInfo":{"status":"ok","timestamp":1613256888852,"user_tz":-360,"elapsed":9440,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBVQvDq8l57ofLdj6VjysLHxPgC3uc6v1yTUoyUA=s64","userId":"13021409582926930931"}},"outputId":"836654f9-80b1-4b07-e303-90a856e43acf"},"source":["train_Q1.shape, train_Q2.shape, train_labels.shape, test_Q1.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((303261,), (303261,), (303261, 2), (101087,))"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"bz7H3hhIN_8T","executionInfo":{"status":"ok","timestamp":1613284358543,"user_tz":-360,"elapsed":684,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}}},"source":["# data generator\n","class data_generator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        sentence_pairs,\n","        labels,\n","        batch_size,\n","        shuffle=True,\n","        include_targets=True,\n","    ):\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        # Load our BERT Tokenizer to encode the text.\n","        # We will use base-base-uncased pretrained model.\n","        self.tokenizer = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\")\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n","        # encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            pad_to_max_length=True,\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.shuffle(self.indexes)\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4O-D3G2OABg","executionInfo":{"status":"ok","timestamp":1613284363922,"user_tz":-360,"elapsed":892,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}}},"source":["# make a model for detecting duplicate question\n","# max_length = 256\n","# learning_rate = 0.001\n","def duplicate_question_detection_model():\n","\n","    # Encoded token ids from BERT tokenizer.\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","\n","    # Attention masks indicates to the model which tokens should be attended to.\n","    attention_masks = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n","    )\n","\n","    # Token type ids are binary masks identifying different sequences in the model.\n","    token_type_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n","    )\n","\n","     # Loading pretrained BERT model.\n","    bert_model = transformers.TFRobertaModel.from_pretrained('roberta-base')\n","\n","    # Freeze the BERT model to reuse the pretrained features without modifying them.\n","    bert_model.trainable = True\n","\n","    outputs = bert_model(\n","        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n","    )\n","\n","    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n","    bi_lstm = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(64, return_sequences=True)\n","    )(outputs.last_hidden_state)\n","\n","    # flat = tf.keras.layers.Flatten()(bi_lstm)\n","\n","    # Applying hybrid pooling approach to bi_lstm sequence output.\n","    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n","    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n","    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n","    dropout = tf.keras.layers.Dropout(0.3)(concat)\n","    output = tf.keras.layers.Dense(2, activation=\"softmax\")(dropout)\n","\n","    # dropout = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n","    # output = tf.keras.layers.Dense(2, activation=\"softmax\")(dropout)\n","\n","    # model object\n","    model = tf.keras.models.Model(\n","        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n","    )\n","\n","    # compile the model\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"acc\"],\n","    )\n","\n","    return model\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["93826d389f8b45b08b7810d0356bee61","fe35fa65907b489da363daab757acb54","456e5bd59f5f459fa7eb04c8a5063cf6","84a4d8f6898440348c135db2d33499ec","cb98d28e8a5b42c4ae04e3ccd23a619d","90d4a2b30b424fce88dc635ad5de6bd2","4f36cba909ea43a5a031dfec7ab9e83c","e33be68bee0f4f999b0e824fe2e3d6ec","3884e00761804355b126c9e206bf0543","f51592efdfec4e958bb89137bba6bdf1","b00a8de65871463085ba1751bfdfde6c","d094c30d6e4042a78d47299553164b33","615434062c784393a4e7fc3319871d0d","ec9b239d9ba14dd0bf3a5def5b52fa39","4b009cb06e174f51821c12792b1f237c","824664bdc46942eaacb8407d470d02d2"]},"id":"Lk_4aHJPOBIU","executionInfo":{"status":"ok","timestamp":1613284432426,"user_tz":-360,"elapsed":64932,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"d6920470-f69c-4fd1-b686-88ec579c9c19"},"source":["# Create the model under a distribution strategy scope.\n","strategy = tf.distribute.MirroredStrategy()\n","\n","with strategy.scope():\n","    max_length=64\n","    learning_rate=5e-5\n","    model = duplicate_question_detection_model()\n","\n","# print(f\"Strategy: {strategy}\")\n","model.summary()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93826d389f8b45b08b7810d0356bee61","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3884e00761804355b126c9e206bf0543","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=657434796.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f19e2f226c8>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: <cyfunction Socket.send at 0x7f1a104022a0> is not a module, class, method, function, traceback, frame, or code object\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f19e2f226c8>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: <cyfunction Socket.send at 0x7f1a104022a0> is not a module, class, method, function, traceback, frame, or code object\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","\n","WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f1a0e1dcc80> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <function wrap at 0x7f1a0e1dcc80> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","attention_masks (InputLayer)    [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","token_type_ids (InputLayer)     [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 124645632   input_ids[0][0]                  \n","                                                                 attention_masks[0][0]            \n","                                                                 token_type_ids[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, 64, 128)      426496      tf_roberta_model[0][0]           \n","__________________________________________________________________________________________________\n","global_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n","                                                                 global_max_pooling1d[0][0]       \n","__________________________________________________________________________________________________\n","dropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 2)            514         dropout_37[0][0]                 \n","==================================================================================================\n","Total params: 125,072,642\n","Trainable params: 125,072,642\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GTN9S3J9OBWe","colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["ab381db9e1874602a3bc993e682c3b2d","68320a4e7a2d410ba31a19112001e7dc","b1623f51ee094b4f9fbd02dd5cc4f5a5","2c4ee3d14a2a4b2499746af97fa689d2","99a084bc648b442db4f221266d628fdd","860972196ba14c86a3005dce545fee8c","a7024971eb1341fca37d562de216ed10","df2db500f1684e2f993feedea32ef94e","47b49953aef744a2a05a3fa1ebd0768b","63a219c4bbc44d00ab81619570879290","732dd4b9b6574b40bb7cce7f54a09a85","65f49d8f1aea44d3951b90d5bb5f4e78","0104d11d9cc54b2da453fb416268bbc5","05866dd8f1d841d9832dfe3cbd8c012a","4be30683d6394bff965b8ebca3bba32e","2c5c8ad1f3b94e6eb42fa3971b30b680"]},"executionInfo":{"status":"ok","timestamp":1613284437431,"user_tz":-360,"elapsed":57632,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"04ed7c10-fb36-439a-9468-51471a04a7f4"},"source":["# data preparation\n","batch_size = 64\n","train_data = data_generator(\n","    data_train[[\"question1\", \"question2\"]].values.astype(\"str\"),\n","    train_labels,\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","valid_data = data_generator(\n","    data_test[[\"question1\", \"question2\"]].values.astype(\"str\"),\n","    test_labels,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab381db9e1874602a3bc993e682c3b2d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47b49953aef744a2a05a3fa1ebd0768b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8A0H_EzJkcmu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613284487656,"user_tz":-360,"elapsed":844,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"b44b31ac-9cc2-4ed3-cbd0-b085ebe110ab"},"source":["# change learning rate\n","\n","tf.keras.backend.set_value(model.optimizer.lr, 1e-5)\n","print(tf.keras.backend.get_value(model.optimizer.lr))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["1e-05\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"16EfvUObOBoy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613286898050,"user_tz":-360,"elapsed":2407753,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"f139d34c-2990-4f1d-e7a2-141936bd2039"},"source":["epochs = 1\n","history = model.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","4738/4738 [==============================] - ETA: 0s - loss: 0.1620 - acc: 0.9369"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","4738/4738 [==============================] - 2406s 503ms/step - loss: 0.1620 - acc: 0.9369 - val_loss: 0.2440 - val_acc: 0.9025\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IlUUr8FxU-B0"},"source":["model.save_weights('/content/drive/My Drive/Duplicate Question Detection/roberta_bi_lstm_pooling_4_epoch_random_0_cleaned_text.h5', overwrite=True, save_format=None, options=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nD0MXJvewvPa","executionInfo":{"status":"ok","timestamp":1613284471689,"user_tz":-360,"elapsed":3794,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}}},"source":["model.load_weights('/content/drive/My Drive/Duplicate Question Detection/roberta_bi_lstm_pooling_4_epoch_random_0_cleaned_text.h5', by_name=False, skip_mismatch=False, options=None)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"95cVHTbO-beQ"},"source":["model.evaluate(valid_data, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZGqDER0QLbQ"},"source":["preds = model.predict(\r\n","    valid_data,\r\n","    batch_size=batch_size,\r\n","    use_multiprocessing=True\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgrtUlFo9wfH"},"source":["from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\r\n","y_pred = np.argmax(preds, axis=1)\r\n","n = y_pred.shape[0]\r\n","y_true = te_labels[:n]\r\n","print('accuracy: ', accuracy_score(y_true, y_pred))\r\n","print('F1 score: ', f1_score(y_true, y_pred))\r\n","print('Recall score: ', recall_score(y_true, y_pred))\r\n","print('confusion matrix: \\n', confusion_matrix(y_true, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXb8ZbnYPlT9"},"source":["# draw curves \n","import matplotlib.pyplot as plt\n","%pylab inline\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='val')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(history.history['acc'], label='train')\n","plt.plot(history.history['val_acc'], label='val')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDCcLnd8pqiH"},"source":[""]},{"cell_type":"code","metadata":{"id":"RJnJPJKcVBcC"},"source":[""],"execution_count":null,"outputs":[]}]}