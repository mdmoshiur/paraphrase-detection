{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"RoBERTa_base_cased_model.ipynb","provenance":[{"file_id":"1-fJsrXsuLr9fRzV-k7Dk2-9pD0KPD9L1","timestamp":1607198858784},{"file_id":"10TWl8ZEOK4aFpXNxKGQ8sNTDe8TbnYee","timestamp":1606471759418},{"file_id":"1XAThkmGIHNtA3Zh2swU8WUNe1CRiwCwO","timestamp":1606250659791}],"collapsed_sections":[],"machine_shape":"hm"},"coursera":{"schema_names":["NLPC3-4A"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"ee0793ae887c453193f556e53e539422":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8fb21e964aaf45bc83a830677766c572","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f8418f46e792405aa51935a381b90d76","IPY_MODEL_c0fdae972d3e4d638812d9b7d55c9165"]}},"8fb21e964aaf45bc83a830677766c572":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f8418f46e792405aa51935a381b90d76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_43e689dceac146c3a306cdc2b5dddcff","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":481,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":481,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bd1374a244f5463ba336f260ba1727e0"}},"c0fdae972d3e4d638812d9b7d55c9165":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6aa83dbcc4f543bcae3ba63e9000403e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 481/481 [00:00&lt;00:00, 5.67kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_03880ae6319240d0bfecde0e87b9f181"}},"43e689dceac146c3a306cdc2b5dddcff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bd1374a244f5463ba336f260ba1727e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6aa83dbcc4f543bcae3ba63e9000403e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"03880ae6319240d0bfecde0e87b9f181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3a722fa5799f427eb939715067e76b82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6757fbc148af4061ae9ead33dc27db5b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_42937f84be694ad3b171633a378d1d85","IPY_MODEL_8c4b8ef39a43406ebf34d946317a8cbf"]}},"6757fbc148af4061ae9ead33dc27db5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42937f84be694ad3b171633a378d1d85":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_56c8e826da504a16be0405e3f70f2962","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":657434796,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":657434796,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a6e4cd8e770343c2aa22e708404ced9d"}},"8c4b8ef39a43406ebf34d946317a8cbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_05cf92ede2ba4f838faa4e6ee6669f2e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 657M/657M [00:11&lt;00:00, 56.5MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bbc6ad0e4ed445c59c1d37da541006f0"}},"56c8e826da504a16be0405e3f70f2962":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a6e4cd8e770343c2aa22e708404ced9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"05cf92ede2ba4f838faa4e6ee6669f2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bbc6ad0e4ed445c59c1d37da541006f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ebf99779c6284e6db616652a4433b6aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_62fbc27601e6494c95d6f461946106b4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b31e6350916d4d05983446141dd95191","IPY_MODEL_e923ea498fc342589b70c06d0c12d90b"]}},"62fbc27601e6494c95d6f461946106b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b31e6350916d4d05983446141dd95191":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6125953440ca402a833cf91567fa15c5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_928888610d344be7b9b748a64f0e8517"}},"e923ea498fc342589b70c06d0c12d90b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6c5b639a18984c36b8170770d3de56a0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 899k/899k [00:00&lt;00:00, 2.61MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ccbf6736b9f44297aac4d39edf6c2ba4"}},"6125953440ca402a833cf91567fa15c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"928888610d344be7b9b748a64f0e8517":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c5b639a18984c36b8170770d3de56a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ccbf6736b9f44297aac4d39edf6c2ba4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"caf18dcad55a4cd785dcee069e7f5f67":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_aa9b1bd1ca2f45d7865315c83fcbe352","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_790ae6e8de7a4e95bd821e1b650fac5a","IPY_MODEL_118c0dad176b4b2190def5460c430df1"]}},"aa9b1bd1ca2f45d7865315c83fcbe352":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"790ae6e8de7a4e95bd821e1b650fac5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dc75d8e019104fe1a07d4e4428357f5f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_64f486e607e341979c77b6091f70fa0e"}},"118c0dad176b4b2190def5460c430df1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2733ffcb527d452dbf0b1d31f3fc9075","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 3.30MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0be022544c37466a8d40ac5fbe3d5b8a"}},"dc75d8e019104fe1a07d4e4428357f5f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"64f486e607e341979c77b6091f70fa0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2733ffcb527d452dbf0b1d31f3fc9075":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0be022544c37466a8d40ac5fbe3d5b8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llTFOgQy9iYj","executionInfo":{"status":"ok","timestamp":1607609417394,"user_tz":-360,"elapsed":5153,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"19cf8620-c7dd-4739-b701-0cfa8461f954"},"source":["# connected GPU information\n","!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Thu Dec 10 14:10:14 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jdG8TWva4fpa"},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XhY0C80ISa-F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607594641830,"user_tz":-360,"elapsed":9178,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"c8883dd0-8461-4203-91d5-838bc4db8abc"},"source":["# install huggingface transformers\n","!pip -q install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.4MB 6.0MB/s \n","\u001b[K     |████████████████████████████████| 890kB 47.3MB/s \n","\u001b[K     |████████████████████████████████| 2.9MB 35.0MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zdACgs491cs2"},"source":["import os\n","\n","import tensorflow as tf\n","import transformers\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","\n","import random as rnd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ui7sHLhTEq2D"},"source":["import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4sF9Hqzgwt0l"},"source":["<a name='1'></a>\n","#Importing the Data"]},{"cell_type":"code","metadata":{"id":"sXWBVGWnpity","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1607594663237,"user_tz":-360,"elapsed":3946,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"a44b2828-1206-4b7e-9a60-3debb77125fc"},"source":["data = pd.read_csv(\"/content/drive/My Drive/Duplicate Question Detection/quora_questions.csv\")\n","#data.fillna(\"none value\", inplace=True) # replace nan value to none\n","# drop the rows with null value\n","data.dropna(axis=0, inplace=True)\n","N=len(data)\n","print('Number of question pairs: ', N)\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of question pairs:  404348\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>qid1</th>\n","      <th>qid2</th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>is_duplicate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>What is the step by step guide to invest in sh...</td>\n","      <td>What is the step by step guide to invest in sh...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n","      <td>What would happen if the Indian government sto...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>How can I increase the speed of my internet co...</td>\n","      <td>How can Internet speed be increased by hacking...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>Why am I mentally very lonely? How can I solve...</td>\n","      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>Which one dissolve in water quikly sugar, salt...</td>\n","      <td>Which fish would survive in salt water?</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  qid1  ...                                          question2 is_duplicate\n","0   0     1  ...  What is the step by step guide to invest in sh...            0\n","1   1     3  ...  What would happen if the Indian government sto...            0\n","2   2     5  ...  How can Internet speed be increased by hacking...            0\n","3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n","4   4     9  ...            Which fish would survive in salt water?            0\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"yMLgpExkBY6g"},"source":["# def cut_to_max(text, max_len):\n","#     words = text.split()[:max_len]\n","#     return ' '.join(words)\n","\n","# # test the func\n","# # cut_to_max(' abc test wewe dfd ddd', 3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"deYmlI46IvjG"},"source":["# data['question1'] = data['question1'].apply(lambda x: cut_to_max(x, 65))\n","# data['question2'] = data['question2'].apply(lambda x: cut_to_max(x, 65))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"da9RE9hnMcej"},"source":["# for s in data['question1']:\n","#     if len(s.split()) > 63:\n","#         print(s)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ktQColfXYdXS","executionInfo":{"status":"ok","timestamp":1607594672620,"user_tz":-360,"elapsed":1710,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"a17e82b3-3c95-4159-8935-d6cf976cf162"},"source":["print(\"dataset labels Distribution\")\n","print(data.is_duplicate.value_counts())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dataset labels Distribution\n","0    255042\n","1    149306\n","Name: is_duplicate, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gkSQTu7Ypit0"},"source":["We first split the data into a train and test set. The test set will be used later to evaluate our model."]},{"cell_type":"code","metadata":{"id":"dkKiI0gip6Qs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607594677494,"user_tz":-360,"elapsed":1324,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"908d55d2-c644-4ac4-9baa-8443818c8db9"},"source":["# model selection\n","from sklearn.model_selection import train_test_split\n","data_train , data_test = train_test_split(data, train_size=0.75, random_state=0)\n","# validation_data, _ = train_test_split(data_train, train_size=0.1, random_state=0)\n","len(data_train), len(data_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(303261, 101087)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"vJQespJVZJKC"},"source":["# print(\"Train dataset Distribution\")\n","# print(data_train.is_duplicate.value_counts())\n","\n","# print(\"\\n\\nTest dataset Distribution\")\n","# print(data_test.is_duplicate.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHpZO58Dss_v"},"source":["train_Q1 = np.array(data_train['question1'])\n","train_Q2 = np.array(data_train['question2'])\n","t_labels = np.array(data_train['is_duplicate'])\n","# make it one-hot encoding\n","train_labels = tf.keras.utils.to_categorical(t_labels, num_classes=2)\n","\n","test_Q1 = np.array(data_test['question1'])\n","test_Q2 = np.array(data_test['question2'])\n","te_labels  = np.array(data_test['is_duplicate'])\n","# make it one-hot encoding\n","test_labels = tf.keras.utils.to_categorical(te_labels, num_classes=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6t-fu-fL-qf4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607594687918,"user_tz":-360,"elapsed":2211,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"f240d310-7195-47e8-c577-741ee44e5c79"},"source":["train_Q1.shape, train_Q2.shape, train_labels.shape, test_Q1.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((303261,), (303261,), (303261, 2), (101087,))"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"bz7H3hhIN_8T"},"source":["# data generator\n","class data_generator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        sentence_pairs,\n","        labels,\n","        batch_size,\n","        shuffle=True,\n","        include_targets=True,\n","    ):\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        # Load our BERT Tokenizer to encode the text.\n","        # We will use base-base-uncased pretrained model.\n","        self.tokenizer = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\")\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n","        # encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            pad_to_max_length=True,\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.shuffle(self.indexes)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4O-D3G2OABg"},"source":["# make a model for detecting duplicate question\n","# max_length = 256\n","# learning_rate = 0.001\n","def duplicate_question_detection_model():\n","\n","    # Encoded token ids from BERT tokenizer.\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","\n","    # Attention masks indicates to the model which tokens should be attended to.\n","    attention_masks = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n","    )\n","\n","    # Token type ids are binary masks identifying different sequences in the model.\n","    token_type_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n","    )\n","\n","     # Loading pretrained BERT model.\n","    bert_model = transformers.TFRobertaModel.from_pretrained('roberta-base', return_dict = False)\n","\n","    # Freeze the BERT model to reuse the pretrained features without modifying them.\n","    bert_model.trainable = True\n","\n","    sequence_output, pooled_output = bert_model(\n","        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n","    )\n","\n","    # # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n","    # bi_lstm = tf.keras.layers.Bidirectional(\n","    #     tf.keras.layers.LSTM(64, return_sequences=True)\n","    # )(sequence_output)\n","\n","    # # Applying hybrid pooling approach to bi_lstm sequence output.\n","    # avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n","    # max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n","    # concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n","    # dropout = tf.keras.layers.Dropout(0.3)(concat)\n","    # output = tf.keras.layers.Dense(2, activation=\"softmax\")(dropout)\n","\n","    dropout = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n","    output = tf.keras.layers.Dense(2, activation=\"softmax\")(dropout)\n","\n","    # model object\n","    model = tf.keras.models.Model(\n","        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n","    )\n","\n","    # compile the model\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"acc\"],\n","    )\n","\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611,"referenced_widgets":["ee0793ae887c453193f556e53e539422","8fb21e964aaf45bc83a830677766c572","f8418f46e792405aa51935a381b90d76","c0fdae972d3e4d638812d9b7d55c9165","43e689dceac146c3a306cdc2b5dddcff","bd1374a244f5463ba336f260ba1727e0","6aa83dbcc4f543bcae3ba63e9000403e","03880ae6319240d0bfecde0e87b9f181","3a722fa5799f427eb939715067e76b82","6757fbc148af4061ae9ead33dc27db5b","42937f84be694ad3b171633a378d1d85","8c4b8ef39a43406ebf34d946317a8cbf","56c8e826da504a16be0405e3f70f2962","a6e4cd8e770343c2aa22e708404ced9d","05cf92ede2ba4f838faa4e6ee6669f2e","bbc6ad0e4ed445c59c1d37da541006f0"]},"id":"Lk_4aHJPOBIU","executionInfo":{"status":"ok","timestamp":1607594737979,"user_tz":-360,"elapsed":26291,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"9a53a594-4da3-49a0-eaa2-e3780f001b5a"},"source":["# Create the model under a distribution strategy scope.\n","strategy = tf.distribute.MirroredStrategy()\n","\n","with strategy.scope():\n","    max_length=64\n","    learning_rate=5e-5\n","    model = duplicate_question_detection_model()\n","\n","# print(f\"Strategy: {strategy}\")\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee0793ae887c453193f556e53e539422","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a722fa5799f427eb939715067e76b82","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=657434796.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","attention_masks (InputLayer)    [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","token_type_ids (InputLayer)     [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","tf_roberta_model (TFRobertaMode ((None, 64, 768), (N 124645632   input_ids[0][0]                  \n","                                                                 attention_masks[0][0]            \n","                                                                 token_type_ids[0][0]             \n","__________________________________________________________________________________________________\n","dropout_37 (Dropout)            (None, 768)          0           tf_roberta_model[0][1]           \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 2)            1538        dropout_37[0][0]                 \n","==================================================================================================\n","Total params: 124,647,170\n","Trainable params: 124,647,170\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GTN9S3J9OBWe","colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["ebf99779c6284e6db616652a4433b6aa","62fbc27601e6494c95d6f461946106b4","b31e6350916d4d05983446141dd95191","e923ea498fc342589b70c06d0c12d90b","6125953440ca402a833cf91567fa15c5","928888610d344be7b9b748a64f0e8517","6c5b639a18984c36b8170770d3de56a0","ccbf6736b9f44297aac4d39edf6c2ba4","caf18dcad55a4cd785dcee069e7f5f67","aa9b1bd1ca2f45d7865315c83fcbe352","790ae6e8de7a4e95bd821e1b650fac5a","118c0dad176b4b2190def5460c430df1","dc75d8e019104fe1a07d4e4428357f5f","64f486e607e341979c77b6091f70fa0e","2733ffcb527d452dbf0b1d31f3fc9075","0be022544c37466a8d40ac5fbe3d5b8a"]},"executionInfo":{"status":"ok","timestamp":1607594747892,"user_tz":-360,"elapsed":5891,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"d534e668-62c2-4c2d-921e-115a2820b9ef"},"source":["# data preparation\n","batch_size = 32\n","train_data = data_generator(\n","    data_train[[\"question1\", \"question2\"]].values.astype(\"str\"),\n","    train_labels,\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","valid_data = data_generator(\n","    data_test[[\"question1\", \"question2\"]].values.astype(\"str\"),\n","    test_labels,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebf99779c6284e6db616652a4433b6aa","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"caf18dcad55a4cd785dcee069e7f5f67","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8A0H_EzJkcmu","executionInfo":{"status":"ok","timestamp":1607598653697,"user_tz":-360,"elapsed":1741,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"0930de4d-b3ad-4f23-8a8e-f0b842b9ae3f"},"source":["# change learning rate\n","\n","tf.keras.backend.set_value(model.optimizer.lr, 2e-5)\n","print(tf.keras.backend.get_value(model.optimizer.lr))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2e-05\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16EfvUObOBoy","executionInfo":{"status":"ok","timestamp":1607601380731,"user_tz":-360,"elapsed":2005606,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqWmRAumhowJ_NmJUinnHkx5ezgWuXabc4kwO1Wg=s64","userId":"10483778837805521382"}},"outputId":"54cc6652-7a5b-4780-f4e7-821f2232c69c"},"source":["epochs = 1\n","history = model.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["9476/9476 [==============================] - 2723s 287ms/step - loss: 0.1483 - acc: 0.9423 - val_loss: 0.2526 - val_acc: 0.9081\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IlUUr8FxU-B0"},"source":["model.save_weights('/content/drive/My Drive/Duplicate Question Detection/roberta_sen_classification_4_epoch.h5', overwrite=True, save_format=None, options=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nD0MXJvewvPa"},"source":["model.load_weights('/content/drive/My Drive/Duplicate Question Detection/roberta_sen_classification_3_epoch.h5', by_name=False, skip_mismatch=False, options=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"95cVHTbO-beQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607558578246,"user_tz":-360,"elapsed":309170,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBVQvDq8l57ofLdj6VjysLHxPgC3uc6v1yTUoyUA=s64","userId":"13021409582926930931"}},"outputId":"a8c00d6e-0150-4e4c-cb86-8f97d55408e4"},"source":["model.evaluate(valid_data, verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["3158/3158 [==============================] - 307s 97ms/step - loss: 0.1390 - acc: 0.9484\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.13897565007209778, 0.9484147429466248]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"xZGqDER0QLbQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607559642435,"user_tz":-360,"elapsed":289648,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBVQvDq8l57ofLdj6VjysLHxPgC3uc6v1yTUoyUA=s64","userId":"13021409582926930931"}},"outputId":"7151081b-b433-4a19-93c6-ca22dae8e09e"},"source":["preds = model.predict(\r\n","    valid_data,\r\n","    batch_size=batch_size,\r\n","    use_multiprocessing=True\r\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n","WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgrtUlFo9wfH","executionInfo":{"status":"ok","timestamp":1607559649532,"user_tz":-360,"elapsed":1274,"user":{"displayName":"Md. Moshiur Rahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBVQvDq8l57ofLdj6VjysLHxPgC3uc6v1yTUoyUA=s64","userId":"13021409582926930931"}},"outputId":"77dfb82b-6304-4ab9-9544-1a0b260118a6"},"source":["from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\r\n","y_pred = np.argmax(preds, axis=1)\r\n","n = y_pred.shape[0]\r\n","y_true = te_labels[:n]\r\n","print('accuracy: ', accuracy_score(y_true, y_pred))\r\n","print('F1 score: ', f1_score(y_true, y_pred))\r\n","print('Recall score: ', recall_score(y_true, y_pred))\r\n","print('confusion matrix: \\n', confusion_matrix(y_true, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy:  0.9073385053831539\n","F1 score:  0.8759866504211474\n","Recall score:  0.8883874607139979\n","confusion matrix: \n"," [[58620  5209]\n"," [ 4155 33072]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cXb8ZbnYPlT9"},"source":["# draw curves \n","import matplotlib.pyplot as plt\n","%pylab inline\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='val')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(history.history['acc'], label='train')\n","plt.plot(history.history['val_acc'], label='val')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDCcLnd8pqiH"},"source":[""]},{"cell_type":"code","metadata":{"id":"RJnJPJKcVBcC"},"source":[""],"execution_count":null,"outputs":[]}]}